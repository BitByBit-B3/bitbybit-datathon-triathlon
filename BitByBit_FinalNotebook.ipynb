{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BitByBit Datathon 2025 - Final Submission Notebook\n",
    "\n",
    "## Team: BitByBit\n",
    "\n",
    "This notebook contains the complete pipeline for:\n",
    "- **Task 1**: Service Processing Time Prediction  \n",
    "- **Task 2**: Staffing Requirements Prediction\n",
    "\n",
    "### Competition Tasks Summary\n",
    "1. **Task 1**: Predict `expected_completion_time_minutes` given date, time, and task_id\n",
    "2. **Task 2**: Predict `predicted_employee_count` given date and section_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# Setup paths\n",
    "code_dir = Path('./code')\n",
    "data_dir = code_dir / 'data' / 'raw'\n",
    "artifacts_dir = code_dir / 'artifacts'\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append(str(code_dir / 'src'))\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Environment setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training datasets\n",
    "try:\n",
    "    bookings_df = pd.read_csv(data_dir / 'bookings_train.csv')\n",
    "    tasks_df = pd.read_csv(data_dir / 'tasks.csv')\n",
    "    staffing_df = pd.read_csv(data_dir / 'staffing_train.csv')\n",
    "    \n",
    "    print(\"Training datasets loaded successfully:\")\n",
    "    print(f\"- Bookings: {len(bookings_df)} records\")\n",
    "    print(f\"- Tasks: {len(tasks_df)} records\")\n",
    "    print(f\"- Staffing: {len(staffing_df)} records\")\n",
    "    \n",
    "    # Display first few rows of each dataset\n",
    "    print(\"\\nBookings Dataset Sample:\")\n",
    "    print(bookings_df.head())\n",
    "    \n",
    "    print(\"\\nTasks Dataset Sample:\")\n",
    "    print(tasks_df.head())\n",
    "    \n",
    "    print(\"\\nStaffing Dataset Sample:\")\n",
    "    print(staffing_df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading datasets: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "### Task 1: Processing Time Target Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import preprocessing functions\n",
    "from preprocessing import clean_bookings_data, merge_training_data\n",
    "from time_utils import parse_datetime_safe, calculate_duration_minutes\n",
    "\n",
    "# Clean bookings data and create target variable for Task 1\n",
    "logger.info(\"Preprocessing bookings data for Task 1...\")\n",
    "\n",
    "# Parse datetime columns\n",
    "bookings_clean = bookings_df.copy()\n",
    "bookings_clean['check_in_time'] = pd.to_datetime(bookings_clean['check_in_time'], errors='coerce')\n",
    "bookings_clean['check_out_time'] = pd.to_datetime(bookings_clean['check_out_time'], errors='coerce')\n",
    "bookings_clean['appointment_date'] = pd.to_datetime(bookings_clean['appointment_date'], errors='coerce')\n",
    "\n",
    "# Calculate processing time target (Task 1)\n",
    "mask = (~bookings_clean['check_in_time'].isna()) & (~bookings_clean['check_out_time'].isna())\n",
    "bookings_clean.loc[mask, 'processing_time_minutes'] = (\n",
    "    bookings_clean.loc[mask, 'check_out_time'] - bookings_clean.loc[mask, 'check_in_time']\n",
    ").dt.total_seconds() / 60\n",
    "\n",
    "# Remove outliers and invalid processing times\n",
    "bookings_clean = bookings_clean[\n",
    "    (bookings_clean['processing_time_minutes'] >= 1) & \n",
    "    (bookings_clean['processing_time_minutes'] <= 480)  # Max 8 hours\n",
    "]\n",
    "\n",
    "print(f\"Valid bookings for Task 1: {len(bookings_clean)}\")\n",
    "print(f\"Processing time statistics:\")\n",
    "print(bookings_clean['processing_time_minutes'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Staffing Target Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare staffing data for Task 2\n",
    "logger.info(\"Preprocessing staffing data for Task 2...\")\n",
    "\n",
    "staffing_clean = staffing_df.copy()\n",
    "staffing_clean['date'] = pd.to_datetime(staffing_clean['date'], errors='coerce')\n",
    "\n",
    "# Remove invalid data\n",
    "staffing_clean = staffing_clean[\n",
    "    (staffing_clean['employees_on_duty'] >= 1) & \n",
    "    (staffing_clean['employees_on_duty'] <= 50)  # Reasonable limits\n",
    "]\n",
    "\n",
    "print(f\"Valid staffing records for Task 2: {len(staffing_clean)}\")\n",
    "print(f\"Employees on duty statistics:\")\n",
    "print(staffing_clean['employees_on_duty'].describe())\n",
    "\n",
    "# Show section distribution\n",
    "print(\"\\nSection distribution:\")\n",
    "print(staffing_clean['section_id'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import feature engineering functions\n",
    "from features_task1 import extract_training_features_task1\n",
    "from features_task2 import extract_training_features_task2\n",
    "\n",
    "# Feature engineering for Task 1\n",
    "logger.info(\"Extracting features for Task 1...\")\n",
    "\n",
    "# Merge bookings with tasks to get section information\n",
    "task1_data = bookings_clean.merge(tasks_df[['task_id', 'section_id']], on='task_id', how='left')\n",
    "\n",
    "# Extract temporal features\n",
    "task1_data['appt_hour'] = pd.to_datetime(task1_data['appointment_time'], format='%H:%M', errors='coerce').dt.hour\n",
    "task1_data['appt_weekday'] = task1_data['appointment_date'].dt.weekday\n",
    "task1_data['appt_month'] = task1_data['appointment_date'].dt.month\n",
    "task1_data['is_weekend'] = task1_data['appt_weekday'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Feature engineering for Task 2\n",
    "logger.info(\"Extracting features for Task 2...\")\n",
    "\n",
    "# Extract temporal features for staffing\n",
    "task2_data = staffing_clean.copy()\n",
    "task2_data['weekday'] = task2_data['date'].dt.weekday\n",
    "task2_data['month'] = task2_data['date'].dt.month\n",
    "task2_data['quarter'] = task2_data['date'].dt.quarter\n",
    "task2_data['is_weekend'] = task2_data['weekday'].isin([5, 6]).astype(int)\n",
    "\n",
    "print(\"Feature engineering completed\")\n",
    "print(f\"Task 1 features shape: {task1_data.shape}\")\n",
    "print(f\"Task 2 features shape: {task2_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "### Task 1: Processing Time Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import baseline models\n",
    "from baselines import MedianBaselinePredictor, train_median_baseline\n",
    "\n",
    "# Train Task 1 model\n",
    "logger.info(\"Training Task 1 model...\")\n",
    "\n",
    "# Prepare features and target\n",
    "task1_features = ['task_id', 'appt_hour', 'appt_weekday', 'section_id']\n",
    "task1_target = 'processing_time_minutes'\n",
    "\n",
    "# Remove rows with missing target or features\n",
    "task1_train = task1_data.dropna(subset=[task1_target] + task1_features)\n",
    "\n",
    "# Create baseline model with hierarchical fallbacks\n",
    "task1_model = MedianBaselinePredictor()\n",
    "\n",
    "# Define grouping strategies (in order of preference)\n",
    "grouping_strategies = [\n",
    "    ['task_id', 'appt_hour', 'appt_weekday'],  # Most specific\n",
    "    ['task_id', 'appt_hour'],\n",
    "    ['task_id', 'appt_weekday'],\n",
    "    ['task_id'],\n",
    "    ['appt_hour', 'appt_weekday'],\n",
    "    ['appt_hour'],\n",
    "    []  # Global median fallback\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "task1_model.fit(task1_train[task1_features], task1_train[task1_target], grouping_strategies)\n",
    "\n",
    "print(f\"Task 1 model trained on {len(task1_train)} samples\")\n",
    "print(f\"Global median: {task1_model.global_median_:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Staffing Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Task 2 model\n",
    "logger.info(\"Training Task 2 model...\")\n",
    "\n",
    "# Prepare features and target\n",
    "task2_features = ['section_id', 'weekday', 'month', 'is_weekend']\n",
    "task2_target = 'employees_on_duty'\n",
    "\n",
    "# Remove rows with missing target or features\n",
    "task2_train = task2_data.dropna(subset=[task2_target] + task2_features)\n",
    "\n",
    "# Create baseline model\n",
    "task2_model = MedianBaselinePredictor()\n",
    "\n",
    "# Define grouping strategies for Task 2\n",
    "grouping_strategies_task2 = [\n",
    "    ['section_id', 'weekday'],  # Most specific\n",
    "    ['section_id', 'is_weekend'],\n",
    "    ['section_id'],\n",
    "    ['weekday'],\n",
    "    []  # Global median fallback\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "task2_model.fit(task2_train[task2_features], task2_train[task2_target], grouping_strategies_task2)\n",
    "\n",
    "print(f\"Task 2 model trained on {len(task2_train)} samples\")\n",
    "print(f\"Global median: {task2_model.global_median_:.1f} employees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Evaluate Task 1 model\n",
    "logger.info(\"Evaluating Task 1 model...\")\n",
    "\n",
    "task1_predictions = task1_model.predict(task1_train[task1_features])\n",
    "task1_mae = mean_absolute_error(task1_train[task1_target], task1_predictions)\n",
    "task1_rmse = np.sqrt(mean_squared_error(task1_train[task1_target], task1_predictions))\n",
    "\n",
    "print(f\"Task 1 Performance:\")\n",
    "print(f\"  MAE: {task1_mae:.2f} minutes\")\n",
    "print(f\"  RMSE: {task1_rmse:.2f} minutes\")\n",
    "\n",
    "# Evaluate Task 2 model\n",
    "logger.info(\"Evaluating Task 2 model...\")\n",
    "\n",
    "task2_predictions = task2_model.predict(task2_train[task2_features])\n",
    "task2_mae = mean_absolute_error(task2_train[task2_target], task2_predictions)\n",
    "task2_rmse = np.sqrt(mean_squared_error(task2_train[task2_target], task2_predictions))\n",
    "\n",
    "print(f\"Task 2 Performance:\")\n",
    "print(f\"  MAE: {task2_mae:.2f} employees\")\n",
    "print(f\"  RMSE: {task2_rmse:.2f} employees\")\n",
    "\n",
    "# Store training metrics\n",
    "training_metrics = {\n",
    "    'task1': {'mae': task1_mae, 'rmse': task1_rmse, 'samples': len(task1_train)},\n",
    "    'task2': {'mae': task2_mae, 'rmse': task2_rmse, 'samples': len(task2_train)}\n",
    "}\n",
    "\n",
    "print(\"\\nModel evaluation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Persistence\n",
    "\n",
    "Save trained models for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create artifacts directory\n",
    "artifacts_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save Task 1 model\n",
    "task1_artifacts = {\n",
    "    'model': task1_model,\n",
    "    'model_type': 'median_baseline',\n",
    "    'feature_columns': task1_features,\n",
    "    'tasks_df': tasks_df,\n",
    "    'training_samples': len(task1_train),\n",
    "    'metrics': training_metrics['task1']\n",
    "}\n",
    "\n",
    "with open(artifacts_dir / 'task1_model.pkl', 'wb') as f:\n",
    "    pickle.dump(task1_artifacts, f)\n",
    "\n",
    "# Save Task 2 model\n",
    "task2_artifacts = {\n",
    "    'model': task2_model,\n",
    "    'model_type': 'median_baseline',\n",
    "    'feature_columns': task2_features,\n",
    "    'training_samples': len(task2_train),\n",
    "    'metrics': training_metrics['task2']\n",
    "}\n",
    "\n",
    "with open(artifacts_dir / 'task2_model.pkl', 'wb') as f:\n",
    "    pickle.dump(task2_artifacts, f)\n",
    "\n",
    "print(\"Models saved to artifacts directory\")\n",
    "print(f\"Task 1 model: {artifacts_dir / 'task1_model.pkl'}\")\n",
    "print(f\"Task 2 model: {artifacts_dir / 'task2_model.pkl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# FINAL INFERENCE CELL\n",
    "\n",
    "## Competition Requirements: Model Loading and Demonstration\n",
    "\n",
    "This cell demonstrates the final model by:\n",
    "1. Loading the saved models (.pkl format)\n",
    "2. Running inference demos for both tasks\n",
    "3. Showing clear input/output examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL INFERENCE DEMONSTRATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL MODEL INFERENCE DEMONSTRATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load trained models from .pkl files\n",
    "print(\"\\n1. Loading trained models...\")\n",
    "\n",
    "# Load Task 1 model\n",
    "with open('task1_model.pkl', 'rb') as f:\n",
    "    task1_artifacts = pickle.load(f)\n",
    "    task1_model_loaded = task1_artifacts['model']\n",
    "    task1_features = task1_artifacts['feature_columns']\n",
    "    tasks_df_loaded = task1_artifacts['tasks_df']\n",
    "\n",
    "print(f\"✓ Task 1 model loaded: {task1_artifacts['model_type']}\")\n",
    "print(f\"  - Trained on {task1_artifacts['training_samples']} samples\")\n",
    "print(f\"  - MAE: {task1_artifacts['metrics']['mae']:.2f} minutes\")\n",
    "\n",
    "# Load Task 2 model\n",
    "with open('task2_model.pkl', 'rb') as f:\n",
    "    task2_artifacts = pickle.load(f)\n",
    "    task2_model_loaded = task2_artifacts['model']\n",
    "    task2_features = task2_artifacts['feature_columns']\n",
    "\n",
    "print(f\"✓ Task 2 model loaded: {task2_artifacts['model_type']}\")\n",
    "print(f\"  - Trained on {task2_artifacts['training_samples']} samples\")\n",
    "print(f\"  - MAE: {task2_artifacts['metrics']['mae']:.2f} employees\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TASK 1: SERVICE PROCESSING TIME PREDICTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Task 1 Inference Demo\n",
    "print(\"\\n2. Task 1 Inference Demo:\")\n",
    "print(\"Input: date, time, task_id\")\n",
    "print(\"Output: expected_completion_time_minutes\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Demo examples for Task 1\n",
    "task1_demo_inputs = [\n",
    "    {'date': '2025-08-29', 'time': '10:30', 'task_id': 'TASK-001'},\n",
    "    {'date': '2025-08-29', 'time': '14:15', 'task_id': 'TASK-008'},\n",
    "    {'date': '2025-08-30', 'time': '09:00', 'task_id': 'TASK-015'}\n",
    "]\n",
    "\n",
    "for i, demo_input in enumerate(task1_demo_inputs, 1):\n",
    "    # Extract features for prediction\n",
    "    appt_datetime = pd.to_datetime(f\"{demo_input['date']} {demo_input['time']}\", errors='coerce')\n",
    "    appt_hour = appt_datetime.hour\n",
    "    appt_weekday = pd.to_datetime(demo_input['date']).weekday()\n",
    "    \n",
    "    # Get section_id from tasks mapping\n",
    "    task_info = tasks_df_loaded[tasks_df_loaded['task_id'] == demo_input['task_id']]\n",
    "    section_id = task_info['section_id'].iloc[0] if len(task_info) > 0 else 'SEC-001'\n",
    "    task_name = task_info['task_name'].iloc[0] if len(task_info) > 0 else 'Unknown Task'\n",
    "    \n",
    "    # Create feature vector\n",
    "    feature_row = pd.DataFrame({\n",
    "        'task_id': [demo_input['task_id']],\n",
    "        'appt_hour': [appt_hour],\n",
    "        'appt_weekday': [appt_weekday],\n",
    "        'section_id': [section_id]\n",
    "    })\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = task1_model_loaded.predict(feature_row[task1_features])[0]\n",
    "    prediction = int(np.round(prediction))\n",
    "    \n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"  Input: date={demo_input['date']}, time={demo_input['time']}, task_id={demo_input['task_id']}\")\n",
    "    print(f\"  Task: {task_name}\")\n",
    "    print(f\"  Predicted completion time: {prediction} minutes\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TASK 2: STAFFING REQUIREMENTS PREDICTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Task 2 Inference Demo\n",
    "print(\"\\n3. Task 2 Inference Demo:\")\n",
    "print(\"Input: date, section_id\")\n",
    "print(\"Output: predicted_employee_count\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Demo examples for Task 2\n",
    "task2_demo_inputs = [\n",
    "    {'date': '2025-08-29', 'section_id': 'SEC-001'},  # Friday\n",
    "    {'date': '2025-08-30', 'section_id': 'SEC-003'},  # Saturday\n",
    "    {'date': '2025-09-01', 'section_id': 'SEC-005'}   # Monday\n",
    "]\n",
    "\n",
    "for i, demo_input in enumerate(task2_demo_inputs, 1):\n",
    "    # Extract features for prediction\n",
    "    demo_date = pd.to_datetime(demo_input['date'])\n",
    "    weekday = demo_date.weekday()\n",
    "    month = demo_date.month\n",
    "    is_weekend = int(weekday in [5, 6])\n",
    "    \n",
    "    # Get section name\n",
    "    section_info = tasks_df_loaded[tasks_df_loaded['section_id'] == demo_input['section_id']]\n",
    "    section_name = section_info['section_name'].iloc[0] if len(section_info) > 0 else 'Unknown Section'\n",
    "    \n",
    "    # Create feature vector\n",
    "    feature_row = pd.DataFrame({\n",
    "        'section_id': [demo_input['section_id']],\n",
    "        'weekday': [weekday],\n",
    "        'month': [month],\n",
    "        'is_weekend': [is_weekend]\n",
    "    })\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = task2_model_loaded.predict(feature_row[task2_features])[0]\n",
    "    prediction = int(np.round(prediction))\n",
    "    \n",
    "    # Day name for clarity\n",
    "    day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    day_name = day_names[weekday]\n",
    "    \n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"  Input: date={demo_input['date']}, section_id={demo_input['section_id']}\")\n",
    "    print(f\"  Section: {section_name}\")\n",
    "    print(f\"  Day: {day_name} ({'Weekend' if is_weekend else 'Weekday'})\")\n",
    "    print(f\"  Predicted employees needed: {prediction}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INFERENCE DEMONSTRATION COMPLETED\")\n",
    "print(\"Models successfully loaded and demonstrated for both tasks\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
